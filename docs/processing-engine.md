# Processing Engine & Logical âš™ï¸

This engine implements a heuristic-driven parsing strategy designed to ingest "messy" human-generated CSVs without manual configuration. It dynamically adapts to unknown delimiters and non-standard layouts (e.g., Vertical Key-Value dumps).

## 1. Pipeline Overview

The processing flow follows a strict `Detect --> rightarrow Decide \rightarrow Parse` execution path:

* 1. â€‹**Dialect Detection:** A statistical sampling of the file determines the likely delimiter and quoting strategy.
* 2. **â€‹Layout Heuristic:** The engine analyzes row width and key repetition to decide if the file is Horizontal (standard tabular) or Vertical (key-value dump).
* 3. â€‹**Adaptive Parsing:** The appropriate parser (Standard DictReader vs. Transposer) is invoked.
* 4. **â€‹Sanitization:** Cell values are scrubbed for Formula Injection risks before memory allocation.

## 2. Dialect Detection (Consistency Measure)

The `DialectDetector` service calculates a **Consistency Score (Q)** for every potential dialect (candidate).

$$Q(D) = P(D) \times T(D)$$

Where:

* **$D$**: The candidate dialect (e.g., delimiter=`,`, quote=`"`).
* **$P(D)$ (Pattern Score)**: Measures how consistent the row lengths are. A file where every row has 5 columns gets a perfect score. A jagged file gets a lower score.
* **$T(D)$ (Type Score)**: Measures how consistent the data types are within columns (e.g., Column 1 is always Date, Column 2 is always Integer).

The candidate with the highest $Q$ score is selected.


â€‹2. Dialect Detection (Consistency Measure)
â€‹We utilize a statistical method to detect dialects rather than a brute-force try-catch approach. The algorithm maximizes a Consistency Score (Q).
â€‹The Algorithm
â€‹For a set of candidate dialects \Theta (permutations of delimiters ,;\t| and quotes '"), we calculate:


## â€‹3. Vertical Layout Transposition (Deep Dive)

â€‹A unique feature of this engine is the ability to ingest Vertical Key-Value streams (often generated by legacy mainframe dumps or debug logs) and transpose them into standard JSON records on the fly.
â€‹3.1 Layout Detection Heuristic
â€‹Before parsing, we check _is_vertical_layout:
â€‹Width Check: Average row width must be \approx 2.
â€‹Repetition Check: The duplication ratio of Column 0 (Keys) must be > 0.3.
â€‹Rationale: In a standard CSV, Column 0 is often a unique ID. In a vertical dump, Column 0 is a Field Name that repeats every N rows.
â€‹3.2 Transposition Logic
â€‹The parse_vertical_csv function operates as a state machine consuming the stream sequentially.
â€‹State:
â€‹current_record: An OrderedDict buffer.
â€‹fields: An insertion-ordered list of discovered keys (Schema).
â€‹The Algorithm:
â€‹Stream Consumption: Read row (K, V).
â€‹Record Boundary Detection:
The engine does not require a blank line or separator. It detects a new record when:
â€‹The current K matches the first detected key of the file (The "Anchor Key").
â€‹AND the current_record already contains this anchor key.
â€‹Flush & Reset:
â€‹If boundary detected: Push current_record to results \rightarrow Reset buffer.
â€‹Schema Evolution:
â€‹If K is unseen, append to fields. This allows later records to introduce new fields without breaking previous ones (though previous ones will lack this key).
â€‹3.3 Edge Cases & Handling
â€‹Missing Values: Rows with only 1 column are treated as (Key, "").
â€‹Schema Drift: If Record 1 has keys [A, B] and Record 2 has [A, B, C], the system adapts. The output schema will be [A, B, C].
â€‹Jagged Vertical Dumps: If a record is missing a specific key (e.g., optional field), it is simply omitted from that specific dictionary. The boundary detection relies solely on the recurrence of the first key.


## 2. Sanitization (Formula Injection)

Attack Vector: CSV Injection (RFC 7111 limitation). Malicious cell values (e.g., =cmd|' /C calc'!A0) can execute code when opened in Excel.

**Sanitization Logic::**
Before any cell is processed, the engine checks if it starts with a trigger character: `=`, `+`, `-`, `@`.

If triggered:

1. The value is escaped by prepending a single quote `'`.
2. `=SUM(1+1)` becomes `'=SUM(1+1)`.
3. Spreadsheets treat this as text, neutralizing the attack.

## 5. Complexity Analysis

### â€‹Time Complexity

| Component | Complexity | Analysis |
| --- | --- | --- |
|Dialect Detection | O(S \cdot C) | S = Sample size (fixed 8KB), C = Candidates (8). Constant time relative to file size. |
|Vertical Check | O(1) | Peeks only the first 20 rows. Negligible overhead. |
|Horizontal Parse | O(N) | Standard Python C-based csv module parsing. Linear scaling with file size. |
|Vertical Parse | O(N) | Single pass. Dictionary lookups for boundary checks are O(1). |
|Sanitization | O(N \cdot M) | M = Avg cell length. Checks are simple character comparisons. |

### Space Complexity & Trade-offs

|Component | Complexity | Trade-off Note|
| --- | ---| --- |
|Parsing (Current) | O(N) High Memory Usage. The current implementation builds the entire List[Dict] in RAM. Large files (>500MB) will trigger OOM.|
|Transposer | O(N) Retains all records. Also accumulates fields list (usually small, O(Columns)).|
|Optimization Path  |  | To support large files, parse_vertical_csv and _parse_csv_sync should be refactored into Python Generators (yield), reducing Space Complexity to O(1) (row-by-row).|

## Future Upgrades

- â€‹Streaming Architecture: Refactor process_csv_content to yield chunks/rows rather than returning a full list, enabling multi-GB file processing.
â€‹- Strict Mode: Add configuration to enforce schema validation (fail on new columns in vertical mode).
â€‹Parquet Export: Direct binary serialization for analytics workloads.
- RFC 4180 compliance parser

##  Reference & Theoretical Basis ðŸ“š

â€‹The dialect detection strategy is strictly based on the Data Consistency Measure proposed in:
> â€‹"Wrangling Messy CSV Files by Detecting Row and Type Patterns"
Gerrit J.J. van den Burg et al. (The Alan Turing Institute), 2018
[arXiv:1811.11242 [cs.DB]]

**â€‹Why this matters:**

Standard Python csv.Sniffer often fails on "messy" files (files with jagged rows or mixed quoting). This implementation calculates a Consistency Score (Q) for potential dialects by multiplying:

* **â€‹Pattern Score (P):** Homogeneity of row lengths.
* **â€‹Type Score (T):** Homogeneity of inferred data types (Integer, Date, String) within columns.

This  implementation adopts the paper's recommendation to prioritize valid substructures over rigid formatting rules, achieving significantly higher accuracy on dirty datasets.
