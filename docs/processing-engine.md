# Processing Engine & Logical âš™ï¸

This engine implements a heuristic-driven parsing strategy designed to ingest "messy" human-generated CSVs without manual configuration. It dynamically adapts to unknown delimiters and non-standard layouts (e.g., Vertical Key-Value dumps).

---

## 1. Pipeline Overview

The processing flow follows a strict `Detect â†’ Decide â†’ Parse` execution path:

1. â€‹**Dialect Detection:** A statistical sampling of the file determines the likely delimiter and quoting strategy.
2. **â€‹Layout Heuristic:** The engine analyzes row width and key repetition to decide if the file is Horizontal (standard tabular) or Vertical (key-value dump).
3. â€‹**Adaptive Parsing:** The appropriate parser (Standard `DictReader` vs. `Transposer`) is invoked.
4. **â€‹Sanitization:** Cell values are scrubbed for Formula Injection risks before memory allocation.

## 2. Dialect Detection (Consistency Measure)

We utilize a statistical method to detect dialects rather than a brute-force try-catch approach. The algorithm maximizes a **Consistency Score (Q)**.

#### The Algorithm

For a set of candidate dialects Î˜ (permutations of delimiters `,;\t|` and quotes `'"`), we calculate:

                    Q(Î¸)=P(x,Î¸)Ã—T(x,Î¸)

Where:

* x: A sample buffer (default 8KB).
* **P (Pattern Score):** Measures row length homogeneity.
* **T (Type Score):** Measures data type homogeneity within columns.

#### Pattern Score (P) logic

We favor files where row lengths (L) are consistent. Penalties are applied for "jagged" CSVs.

                P=K1â€‹k=1âˆ‘Kâ€‹Nkâ€‹Lkâ€‹Lkâ€‹âˆ’1â€‹

* K: Number of distinct row length patterns found.
* Nkâ€‹: Count of rows with length Lkâ€‹.
* Note: Single-column results are heavily penalized (Lkâ€‹â‰ˆ1) to prevent false positives on plain text files.

#### Type Score (T) logic

We infer types using a compiled regex precedence list (Integer â†’ Float â†’ Date â†’ String).

                T=M1â€‹cellsâˆ‘â€‹I(cellâˆˆKnownTypes)

* M: Total number of cells.
* This prevents a "valid" CSV structure (perfect columns) containing garbage data from winning over a slightly "messy" CSV containing coherent data.

## 3. Vertical Layout Transposition

A unique feature of this engine is the ability to ingest Vertical Key-Value streams (often generated by legacy mainframe dumps or debug logs) and transpose them into standard JSON records on the fly.

### 3.1 Layout Detection Heuristic

Before parsing, we `check _is_vertical_layout`:

* Width Check: Average row width must be â‰ˆ2.
* Repetition Check: The duplication ratio of Column 0 (Keys) must be >0.3.
  * Rationale: In a standard CSV, Column 0 is often a unique ID. In a vertical dump, Column 0 is a Field Name that repeats every N rows.

### 3.2 Transposition Logic

The `parse_vertical_csv` function operates as a state machine consuming the stream sequentially.

State:

* `current_record`: An `OrderedDict` buffer.
* `fields: An insertion-ordered list of discovered keys (Schema).

#### The Algorithm:

1. Stream Consumption: Read row (K,V).
2. Record Boundary Detection: The engine does not require a blank line or separator. It detects a new record when:
    * The current K matches the first detected key of the file (The "Anchor Key").
    * AND the current_record already contains this anchor key.
3. Flush & Reset:
    * If boundary detected: Push current_record to results â†’ Reset buffer.

4. **Schema Evolution:**
    * If K is unseen, append to fields. This allows later records to introduce new fields without breaking previous ones (though previous ones will lack this key).

### 3.3 Edge Cases & Handling

* Missing Values: Rows with only 1 column are treated as (Key, "").
* Schema Drift: If Record 1 has keys [A, B] and Record 2 has [A, B, C], the system adapts. The output schema will be [A, B, C].
* Jagged Vertical Dumps: If a record is missing a specific key (e.g., optional field), it is simply omitted from that specific dictionary. The boundary detection relies solely on the recurrence of the first key.

## 4. Sanitization (Formula Injection)

Attack Vector: CSV Injection (RFC 7111 limitation). Malicious cell values (e.g., =cmd|' /C calc'!A0) can execute code when opened in Excel.

Sanitization Logic: We treat the CSV as untrusted user input.

Trigger Detection: Check if cell starts with =, +, -, @.

Neutralization: Prepend a single quote '.

Input: =SUM(1+1)

Output: '=SUM(1+1) (Rendered as literal text by spreadsheets).

## 5. Complexity Analysis

### â€‹Time Complexity

| Component | Complexity | Analysis |
| --- | --- | --- |
|Dialect Detection | O(Sâ‹…C) | S = Sample size (fixed 8KB), C = Candidates (8). Constant time relative to file size. |
|Vertical Check | O(1) | Peeks only the first 20 rows. Negligible overhead. |
|Horizontal Parse | O(N) | Standard Python C-based csv module parsing. Linear scaling with file size. |
|Vertical Parse | O(N) | Single pass. Dictionary lookups for boundary checks are O(1). |
|Sanitization |O(Nâ‹…M) | M = Avg cell length. Checks are simple character comparisons. |

### Space Complexity & Trade-offs

|Component | Complexity | Trade-off Note|
| --- | ---| --- |
|Parsing (Current) | O(N) | High Memory Usage. The current implementation builds the entire List[Dict] in RAM. Large files (>500MB) will trigger OOM.|
|Transposer | O(N) | Retains all records. Also accumulates fields list (usually small, O(Columns)).|
|Optimization Path  |  | To support large files, parse_vertical_csv and _parse_csv_sync should be refactored into Python Generators (yield), reducing Space Complexity to O(1) (row-by-row).|

---

## 6. Future Upgrades

* â€‹Streaming Architecture: Refactor process_csv_content to yield chunks/rows rather than returning a full list, enabling multi-GB file processing.
* Strict Mode: Add configuration to enforce schema validation (fail on new columns in vertical mode).
â€‹Parquet Export: Direct binary serialization for analytics workloads.
* RFC 4180 compliance parser

---

## 7. Reference & Theoretical Basis ðŸ“š

â€‹The dialect detection strategy is strictly based on the Data Consistency Measure proposed in:
> â€‹"Wrangling Messy CSV Files by Detecting Row and Type Patterns"
Gerrit J.J. van den Burg et al. (The Alan Turing Institute), 2018
[arXiv:1811.11242 [cs.DB]]

Standard Python csv.Sniffer often fails on "messy" files (files with jagged rows or mixed quoting). This implementation calculates a Consistency Score (Q) for potential dialects by multiplying:

* **â€‹Pattern Score (P):** Homogeneity of row lengths.
* **â€‹Type Score (T):** Homogeneity of inferred data types (Integer, Date, String) within columns.

This  implementation adopts the paper's recommendation to prioritize valid substructures over rigid formatting rules, achieving significantly higher accuracy on dirty datasets.
